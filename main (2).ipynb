{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3aeaa7e",
   "metadata": {},
   "source": [
    "1_Define the problem: \n",
    "\n",
    "    The problem is to develop a machine learning model that can accurately classify Reddit posts as fake news or not based on \n",
    "    their title. This task can help mitigate the spread of false information on social media platforms.\n",
    "\n",
    "2_What is the input? \n",
    "\n",
    "The input is a text string that represents the title of a Reddit post. The text may contain various \n",
    "forms of words, including slang, misspellings, abbreviations, and other variations that can make text classification challenging.\n",
    "\n",
    "3_What is the output? \n",
    "\n",
    "The output is a binary classification label that indicates whether the Reddit post is fake news or not. \n",
    "This label can be represented as a 0 or 1, where 0 represents a genuine post and 1 represents a fake news post.\n",
    "\n",
    "4_What data mining function is required? \n",
    "\n",
    "A text classification algorithm is required to perform the classification task. This involves preprocessing the text data, extracting relevant features, \n",
    "and training a machine learning model to classify the posts. Some of the data mining techniques that can be used in this task include text preprocessing, \n",
    "feature engineering, model selection, and hyperparameter tuning.\n",
    "\n",
    "5_What could be the challenges? \n",
    "\n",
    "Some of the challenges include dealing with the unstructured nature of the text data, identifying relevant features that can\n",
    "help discriminate between fake and real news, and addressing issues of class imbalance or bias in the dataset. Additionally, the text data may contain noise, \n",
    "ambiguity, and subjectivity that can make it difficult to develop a robust classification model.\n",
    "\n",
    "6_What is the impact? \n",
    "\n",
    "The impact of this project is to help identify and mitigate the spread of fake news on social media platforms, which can have significant social and political consequences.\n",
    "By accurately identifying fake news posts, this project can help prevent the spread of misinformation, promote media literacy, and enhance the credibility of online information\n",
    "sources.\n",
    "\n",
    "7_What is an ideal solution? \n",
    "\n",
    "Model: Random Forest\n",
    "\n",
    "Validation : Random Search\n",
    "\n",
    "Vectorizer: tfidf(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f66b8b",
   "metadata": {},
   "source": [
    "8-What is the difference between Character n-gram and Word n-gram?  Which one tends to suffer more from the OOV issue?\n",
    "\n",
    "Character n-grams are sequences of characters of length n, whereas word n-grams are sequences of words of length n. Character n-grams are useful for capturing information about the morphology and spelling of words, while word n-grams capture information about the semantics and syntax of language.\n",
    "\n",
    "Character n-grams tend to suffer more from the OOV (out-of-vocabulary) issue, as they can generate a large number of n-grams that may not be present in the training data. This is because words can be spelled in different ways and may contain misspellings, abbreviations, and other variations that can increase the number of unique n-grams. Word n-grams, on the other hand, tend to be less affected by the OOV issue, as they are based on the presence or absence of whole words, which are more likely to be present in the training data.\n",
    "\n",
    "9-What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "\n",
    "Stop word removal is the process of removing frequently occurring words, such as \"the\", \"and\", and \"a\", from a text corpus. These words are not informative for text classification tasks and can be safely removed without losing valuable information.\n",
    "\n",
    "Stemming, on the other hand, is the process of reducing words to their base or root form by removing suffixes and prefixes. For example, the words \"running\", \"runs\", and \"run\" would be reduced to the base form \"run\". Stemming can help reduce the dimensionality of the data and improve the accuracy of text classification models.\n",
    "\n",
    "Both techniques are language-dependent, as the list of stop words and the rules for stemming can vary depending on the language and the context of the text corpus. For example, stop words in English may differ from stop words in Spanish, and the rules for stemming may need to be adapted to account for irregular verbs and noun forms in different languages.\n",
    "\n",
    "10-Is tokenization techniques language dependent? Why?\n",
    "\n",
    "Tokenization is the process of breaking down a text corpus into individual units, or tokens, such as words, punctuation marks, or other symbols. Tokenization techniques can be language-dependent, as different languages may have different rules for tokenizing text.\n",
    "\n",
    "For example, in English, words are typically separated by spaces, while in languages like Chinese and Japanese, there may be no spaces between words. Similarly, in some languages, such as Arabic and Hebrew, words are written from right to left instead of left to right, which can affect the tokenization process.\n",
    "\n",
    "Tokenization techniques may also need to be adapted to account for the specific context of the text corpus, such as the presence of abbreviations, acronyms, or other non-standard forms of text.\n",
    "\n",
    "11-What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "\n",
    "Count vectorizer is a technique for converting a text corpus into a matrix of word counts, where each row represents a document and each column represents a unique word in the corpus. Tf-idf vectorizer, on the other hand, is a technique for weighting the word counts based on their frequency and importance in the corpus. Tf-idf vectorizer assigns higher weights to words that are more frequent in a particular document but less frequent in the overall corpus.\n",
    "\n",
    "It would not be feasible to use all possible n-grams, as this would generate a very large number of features and increase the computational complexity of the classification task. Instead, it is common to limit the number of n-grams based on their frequency or information gain, or to use a technique such as feature selection or dimensionality reduction to identify the most informative n-grams.\n",
    "\n",
    "To select the most informative n-grams, one approach is to use techniques such as mutual information or chi-squared tests to identify the n-grams that are most strongly associated with the target variable. Another approach is to use domain knowledge or heuristic rules to select n-grams that are likely to be informative based on the specific context of the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db91c914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1003\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1003\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.1.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1003\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.1.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import nltk \n",
    "from bokeh.io import output_notebook\n",
    "import scipy.stats as stats\n",
    "output_notebook()\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# some seeting for pandas and hvplot\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 300\n",
    "pd.options.display.max_colwidth = 100\n",
    "np.set_printoptions(threshold=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8842d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d3f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stargazer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PD: Phoenix car thief gets instructions from YouTube video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His Own Credibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>59146</td>\n",
       "      <td>Bicycle taxi drivers of New Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>59147</td>\n",
       "      <td>Trump blows up GOP's formula for winning House races</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>59148</td>\n",
       "      <td>Napoleon returns from his exile on the island of Elba. (March 1815), Colourised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>59149</td>\n",
       "      <td>Deep down he always wanted to be a ballet dancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>59150</td>\n",
       "      <td>Toddler miraculously survives 6-story fall landing on car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  \\\n",
       "0          0   \n",
       "1          1   \n",
       "2          2   \n",
       "3          3   \n",
       "4          4   \n",
       "...      ...   \n",
       "59146  59146   \n",
       "59147  59147   \n",
       "59148  59148   \n",
       "59149  59149   \n",
       "59150  59150   \n",
       "\n",
       "                                                                                  text  \n",
       "0                                                                           stargazer   \n",
       "1                                                                                 yeah  \n",
       "2                           PD: Phoenix car thief gets instructions from YouTube video  \n",
       "3                       As Trump Accuses Iran, He Has One Problem: His Own Credibility  \n",
       "4                                                         \"Believers\" - Hezbollah 2011  \n",
       "...                                                                                ...  \n",
       "59146                                                Bicycle taxi drivers of New Delhi  \n",
       "59147                             Trump blows up GOP's formula for winning House races  \n",
       "59148  Napoleon returns from his exile on the island of Elba. (March 1815), Colourised  \n",
       "59149                                 Deep down he always wanted to be a ballet dancer  \n",
       "59150                        Toddler miraculously survives 6-story fall landing on car  \n",
       "\n",
       "[59151 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('xy_train.csv', sep=\",\", na_values=[\"\"])\n",
    "df_test = pd.read_csv('x_test.csv', sep=\",\", na_values=[\"\"])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567965b",
   "metadata": {},
   "source": [
    "# Preprocessing performed:\n",
    "1- Lemmatization to resize words to their base form.\n",
    "\n",
    "2- URL removal and html tags\n",
    "\n",
    "3- Lowercase conversion\n",
    "\n",
    "4- remove punctuation and stop words\n",
    "\n",
    "5- remove extra white spaces\n",
    "\n",
    "6- remove single letter chars\n",
    "\n",
    "7-remove single letter chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2af7a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aly\n",
      "[nltk_data]     Abdelkader\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aly\n",
      "[nltk_data]     Abdelkader\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aly\n",
      "[nltk_data]     Abdelkader\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#helper function for preocessing\n",
    "def clean_text(text, for_embedding=False):\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a1a9d",
   "metadata": {},
   "source": [
    "Create a new column containing the updated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2bb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train text\n",
    "df_train[\"text_clean\"] = df_train[\"text\"]\n",
    "df_train[\"text_clean\"] = df_train[\"text_clean\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")\n",
    "# Clean test text\n",
    "df_test[\"text_clean\"] = df_test[\"text\"]\n",
    "df_test[\"text_clean\"] = df_test[\"text_clean\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c445fd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32172\n",
       "1    27596\n",
       "2      232\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd60a74",
   "metadata": {},
   "source": [
    "there appears to be an extra class with little frequency so we will remove it to turn it to a binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fc750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all class 2 to null\n",
    "df_train.loc[df_train[\"label\"]==2] = np.NaN\n",
    "\n",
    "\n",
    "# Drop when any of x missing\n",
    "df_train = df_train[(df_train[\"text_clean\"] != \"\") & (df_train[\"text_clean\"] != \"null\")]\n",
    "\n",
    "\n",
    "df_train = df_train.dropna(\n",
    "    axis=\"index\", subset=[\"label\", \"text\", \"text_clean\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c54508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one          3285\n",
       "new          2998\n",
       "like         2949\n",
       "man          2706\n",
       "trump        2577\n",
       "colorized    2430\n",
       "people       2315\n",
       "first        2247\n",
       "old          2222\n",
       "look         2214\n",
       "say          2147\n",
       "get          2072\n",
       "time         2011\n",
       "poster       1999\n",
       "found        1959\n",
       "day          1935\n",
       "woman        1892\n",
       "war          1858\n",
       "life         1769\n",
       "make         1727\n",
       "world        1570\n",
       "u            1506\n",
       "american     1498\n",
       "psbattle     1468\n",
       "state        1387\n",
       "post         1384\n",
       "two          1364\n",
       "school       1339\n",
       "back         1325\n",
       "photo        1324\n",
       "made         1314\n",
       "right        1301\n",
       "circa        1249\n",
       "child        1216\n",
       "know         1201\n",
       "president    1199\n",
       "see          1181\n",
       "house        1175\n",
       "way          1164\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "data_clean=df_train.copy()\n",
    "# Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(data_clean[\"text_clean\"]).split()).value_counts()\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2211b7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59151, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d5ed7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veja</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pibulsonggram</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baring</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jockstrap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybernate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tardigrade</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>upriver</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rohl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>squinted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wahre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  freq\n",
       "0           veja     1\n",
       "1  pibulsonggram     1\n",
       "2         baring     1\n",
       "3      jockstrap     1\n",
       "4      hybernate     1\n",
       "5     tardigrade     1\n",
       "6        upriver     1\n",
       "7           rohl     1\n",
       "8       squinted     1\n",
       "9          wahre     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most uncommon words\n",
    "word_freq[-10:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcba854b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.538221\n",
       "1.0    0.461779\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings\n",
    "data_clean[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7747e20a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59758,)\n",
      "(59151,)\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "X_train = df_train[\"text_clean\"]\n",
    "Y_train = df_train[\"label\"]\n",
    "X_test = df_test[\"text_clean\"]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb98f34",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "This tunable pipeline contains models and vectorizers of choice used for all our Trials.\n",
    "\n",
    "* Models: Logistic Reggression, Passive Aggressive Classifier , XGBoost Classifier, Random Forest .\n",
    "\n",
    "* Vectorizers: TfidfVectorizer , Count Vectorizer.\n",
    "* HyperParameter Optimization: Grid search, Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf5b8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfid Vectorizer\n",
    "preprocessortf = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer())]\n",
    ")\n",
    "#Count vectorizer\n",
    "preprocessorcnt = Pipeline(\n",
    "    steps=[\n",
    "        (\"count\", CountVectorizer())]\n",
    ")\n",
    "\n",
    "#logistic Reggression\n",
    "full_piplineLOG = Pipeline(  \n",
    "    steps=[\n",
    "        ('preprocessor', preprocessorcnt), \n",
    "        ('my_classifier', \n",
    "           LogisticRegression(), # Logistic Reggression.\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "#xgboost\n",
    "full_piplineXGB = Pipeline(  \n",
    "    steps=[\n",
    "        ('preprocessor', preprocessortf), \n",
    "        ('my_classifier', \n",
    "           XGBClassifier(), # XGBClassifier.\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "#passive aggrisive Classifier\n",
    "full_piplinePAC = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessortf),\n",
    "        ('my_classifier', \n",
    "           PassiveAggressiveClassifier(),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "full_piplineRAN = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessortf),\n",
    "        ('my_classifier', \n",
    "           RandomForestClassifier(),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd089ba",
   "metadata": {},
   "source": [
    "# Trial 1\n",
    "we will use Logistic reggression with tfid Vectorizer and tune hyper parameters using grid random search.\n",
    "* LogisticReggression: it's a perfect classifier to start with for it's simplicity and suitability for binary classification\n",
    "\n",
    "\n",
    "* tfid: TF-IDF stands for “term frequency-inverse document frequency”, meaning the weight assigned to each token not only depends on its frequency in a document but also how recurrent that term is in the entire corpora. \n",
    "\n",
    "\n",
    "* Random search: since it doesn't take too long we will use it to determine if the model is promising and worth using grid search for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f6a0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "best score 0.8716812338211024\n",
      "best score {'preprocessor__tfidf__ngram_range': (1, 5), 'preprocessor__tfidf__min_df': 6, 'preprocessor__tfidf__max_df': 0.3, 'my_classifier__solver': 'lbfgs', 'my_classifier__penalty': 'l2', 'my_classifier__C': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Define Parameters\n",
    "params = {\n",
    "    \"preprocessor__tfidf__ngram_range\": [(1, 5), (1, 3)],\n",
    "    \"preprocessor__tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"preprocessor__tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__penalty': ['l2'],#l2 regularization\n",
    "          'my_classifier__C' : [1.4,1.6,1.8,2.0],#The parameter C is the the inverse of regularization strength in Logistic Regression\n",
    "          'my_classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "log_rnd = RandomizedSearchCV(\n",
    "    full_piplineLOG, params, cv=5, verbose=1, n_jobs=-1, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "log_rnd.fit(X_train, Y_train)\n",
    "\n",
    "print('best score {}'.format(log_rnd.best_score_))\n",
    "print('best score {}'.format(log_rnd.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5f448",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "the the model had decent start with accuracy of 87% using tfidvectorizer and random search \n",
    "in the next Trial we will Try using count Vectorizer and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1884c",
   "metadata": {},
   "source": [
    "# Trial 2\n",
    "logistic reggression had promising perfrormance so we will use it again this time with grid search to try all posssibe hyperparameter combination and we will also try Count vectorizer.\n",
    "\n",
    "\n",
    "* Count Vectorizer: it counts the number of times a token shows up in the document and uses this value as its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c116984c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1350 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.8592968  0.85929508 0.85884884 0.85885141 0.85837319 0.85836865\n",
      " 0.85791143 0.85790606 0.85744071 0.85744006 0.85714351 0.85714524\n",
      " 0.85703027 0.85703782 0.85695675 0.85696327 0.85658384 0.85659326\n",
      " 0.85625293 0.8562611  0.85622701 0.85623279 0.85600425 0.85600991\n",
      " 0.85623148 0.85623589 0.85590824 0.85591285 0.85518917 0.8551923\n",
      " 0.85937792 0.85931917 0.85884463 0.8589016  0.85843353 0.85836226\n",
      " 0.85805143 0.85797345 0.85744345 0.85748838 0.85715328 0.85714281\n",
      " 0.85696663 0.8570722  0.85685102 0.85689128 0.85649621 0.85660994\n",
      " 0.85615555 0.85625151 0.85618324 0.85617033 0.85594113 0.85586819\n",
      " 0.85612078 0.8561737  0.8558212  0.8558222  0.85509997 0.8551352\n",
      " 0.85929869 0.85929618 0.85884937 0.85885089 0.85837436 0.85836921\n",
      " 0.85791218 0.85790693 0.85744085 0.85744008 0.85714385 0.8571455\n",
      " 0.85703024 0.85703829 0.85695723 0.85696376 0.8565838  0.85659328\n",
      " 0.85625284 0.85626163 0.85622687 0.85623278 0.85600463 0.85600974\n",
      " 0.85623215 0.85623632 0.8559081  0.85591271 0.85518967 0.85519253\n",
      " 0.83612725 0.83834744 0.83586711 0.83808913 0.83557376 0.83776765\n",
      " 0.83530207 0.83749354 0.83479045 0.83696856 0.83454021 0.83670249\n",
      " 0.83431735 0.83646697 0.83417149 0.83631667 0.8338039  0.83594583\n",
      " 0.83344102 0.83558696 0.83310284 0.83523133 0.83285506 0.83499506\n",
      " 0.83284918 0.83495926 0.83252736 0.83463857 0.83215031 0.83423842\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.85743999 0.85743749 0.85743759 0.8574457  0.8575495  0.85756233\n",
      " 0.85739359 0.85739874 0.85707471 0.85707714 0.8570117  0.85700493\n",
      " 0.85723686 0.85722779 0.85731057 0.85730634 0.8571185  0.85710858\n",
      " 0.85673687 0.85674714 0.85680678 0.85679575 0.85661594 0.85662185\n",
      " 0.85690833 0.85690731 0.85663095 0.85663684 0.85590807 0.85589793\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.85818975 0.85818895 0.85775711 0.85776112 0.85732852 0.85732557\n",
      " 0.85696463 0.8569604  0.85654191 0.85653991 0.85630072 0.85630259\n",
      " 0.85625779 0.85626331 0.85624571 0.85625235 0.85592294 0.85593289\n",
      " 0.85563722 0.85564681 0.8556587  0.85566486 0.85547305 0.85547798\n",
      " 0.85574755 0.85575168 0.85546844 0.85547166 0.85476324 0.85476718\n",
      " 0.85831481 0.85815953 0.85790139 0.85786635 0.85736287 0.85730876\n",
      " 0.85701045 0.8569744  0.85647484 0.8565077  0.85626584 0.85634708\n",
      " 0.85626886 0.85617044 0.85612938 0.85622319 0.85578689 0.8558894\n",
      " 0.85551387 0.85556291 0.85562047 0.85565304 0.8553986  0.85544115\n",
      " 0.85572492 0.855672   0.85531519 0.85542518 0.85469138 0.85473552\n",
      " 0.85819041 0.85818933 0.85775784 0.85776138 0.85732889 0.85732623\n",
      " 0.8569648  0.85696136 0.85654206 0.85654016 0.85630112 0.85630307\n",
      " 0.85625827 0.856264   0.85624599 0.85625281 0.85592359 0.85593342\n",
      " 0.85563723 0.85564694 0.85565874 0.85566538 0.85547318 0.85547814\n",
      " 0.85574852 0.85575224 0.85546845 0.85547205 0.85476367 0.85476748\n",
      " 0.83605183 0.83827765 0.83582104 0.83803064 0.83550826 0.83772687\n",
      " 0.83525732 0.83743556 0.83471718 0.83692218 0.83446863 0.83663705\n",
      " 0.83424573 0.83641902 0.83411818 0.83626397 0.83374053 0.83589852\n",
      " 0.83338931 0.83553587 0.83301724 0.83515014 0.83277671 0.83490248\n",
      " 0.83276179 0.83488066 0.83245711 0.83456673 0.83207806 0.8341849\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8556964  0.85569376 0.85576957 0.855782   0.85597047 0.85597145\n",
      " 0.85600907 0.85600461 0.85580458 0.85580679 0.85586497 0.85585201\n",
      " 0.85616886 0.8561634  0.85632904 0.85633256 0.85623738 0.85623953\n",
      " 0.85592161 0.85591632 0.85604986 0.85604617 0.85592744 0.85593223\n",
      " 0.85627227 0.85626691 0.85604075 0.85603794 0.85535826 0.85535961\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8571314  0.8571296  0.8567552  0.85675708 0.85638448 0.85637791\n",
      " 0.85611368 0.85610978 0.85572409 0.85572534 0.85554071 0.85554449\n",
      " 0.85556986 0.85557649 0.85561234 0.85561928 0.85533431 0.85534223\n",
      " 0.85509104 0.85509984 0.85516205 0.85516655 0.8550086  0.85501382\n",
      " 0.85532629 0.85533105 0.85508243 0.85508784 0.85439637 0.85440034\n",
      " 0.85713873 0.85730718 0.8567909  0.85681356 0.85646838 0.85639324\n",
      " 0.85619551 0.85614767 0.85575084 0.85570092 0.85547467 0.85556728\n",
      " 0.85565908 0.85547444 0.85563069 0.85551713 0.85522677 0.85533335\n",
      " 0.854925   0.85505769 0.85497746 0.85505634 0.85485665 0.85486716\n",
      " 0.85523891 0.85515553 0.8549792  0.85502334 0.85414873 0.85428715\n",
      " 0.85713175 0.85712974 0.85675507 0.85675729 0.85638506 0.85637905\n",
      " 0.8561149  0.85611061 0.85572449 0.85572546 0.85554084 0.8555452\n",
      " 0.85557012 0.85557646 0.85561283 0.85561863 0.85533466 0.85534257\n",
      " 0.85509125 0.85509953 0.8551619  0.85516682 0.85500907 0.85501401\n",
      " 0.85532643 0.85533122 0.85508263 0.85508819 0.85439637 0.8544005\n",
      " 0.83600033 0.83823657 0.83576599 0.83796425 0.83546843 0.83768875\n",
      " 0.83521651 0.83739563 0.83467498 0.83684569 0.83442544 0.83659626\n",
      " 0.83418325 0.83636267 0.83406763 0.836219   0.83370138 0.83584021\n",
      " 0.83333825 0.83549172 0.83293899 0.83509158 0.8327068  0.83483802\n",
      " 0.83270638 0.83482792 0.83241215 0.83451449 0.83202583 0.83412708\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.85394355 0.85395774 0.85417648 0.85417897 0.85446647 0.85445946\n",
      " 0.85470164 0.85472368 0.85461064 0.85459601 0.85480256 0.85480036\n",
      " 0.85520675 0.85520586 0.85542944 0.85545098 0.85543035 0.85542874\n",
      " 0.8551902  0.85519518 0.85537603 0.85537827 0.85534018 0.85534235\n",
      " 0.8557185  0.85571249 0.85554701 0.85555425 0.85487445 0.85487864\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8593779248470911\n",
      "best score {'my_classifier__C': 1.6, 'my_classifier__penalty': 'l2', 'my_classifier__solver': 'lbfgs', 'preprocessor__count__max_df': 0.3, 'preprocessor__count__min_df': 5, 'preprocessor__count__ngram_range': (1, 5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "params_cnt_log = {\n",
    "    \n",
    "    \"preprocessor__count__ngram_range\": [(1, 5), (1, 3)],\n",
    "    \"preprocessor__count__max_df\": np.arange(0.3, 1),\n",
    "    \"preprocessor__count__min_df\": np.arange(5,20),\n",
    "    'my_classifier__penalty': ['l2','l1'],#l2 regularization\n",
    "          'my_classifier__C' : [1.6,1.8,2.0],#The parameter C is the the inverse of regularization strength in Logistic Regression\n",
    "          'my_classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "}\n",
    "log_grid = GridSearchCV(\n",
    "    full_piplineLOG, params_cnt_log, cv=5, verbose=1, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "log_grid.fit(X_train, Y_train)\n",
    "\n",
    "print('best score {}'.format(log_grid.best_score_))\n",
    "print('best score {}'.format(log_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532fe2e",
   "metadata": {},
   "source": [
    "Oservation:\n",
    "looks like some fits failed the search and the reason is i included \"L1\" penality and some solvers don't support it.\n",
    "in case of count vectorizer it under performes tfidf so we will continue with tfidf in the next Trials and we will try character level vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01133829",
   "metadata": {},
   "source": [
    "# Trial 3\n",
    "for the next 2 trial we will try xgboost with Random search first then with Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eccdc40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "best score 0.8490724708783874\n",
      "best score {'preprocessor__tfidf__ngram_range': (1, 5), 'preprocessor__tfidf__min_df': 5, 'preprocessor__tfidf__max_df': 0.3, 'my_classifier__subsample': 0.8, 'my_classifier__n_estimators': 450, 'my_classifier__max_depth': 15, 'my_classifier__learning_rate': 0.1, 'my_classifier__gamma': 0}\n"
     ]
    }
   ],
   "source": [
    "xgb_param = {\n",
    "    \"preprocessor__tfidf__ngram_range\": [(1, 5)],\n",
    "    \"preprocessor__tfidf__max_df\": np.arange(0.3,0.4),\n",
    "    \"preprocessor__tfidf__min_df\": np.arange(5,7),\n",
    "  'my_classifier__n_estimators': [425,450],\n",
    "  'my_classifier__max_depth': [12,15],\n",
    "  'my_classifier__learning_rate': [0.1],\n",
    "  'my_classifier__gamma': [0,0.5],\n",
    "  'my_classifier__subsample': [0.5,0.8],\n",
    "}\n",
    "xgb_rnd = RandomizedSearchCV(\n",
    "    full_piplineXGB, xgb_param, cv=5, verbose=1, n_jobs=-1, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "xgb_rnd.fit(X_train, Y_train)\n",
    "\n",
    "print('best score {}'.format(xgb_rnd.best_score_))\n",
    "print('best score {}'.format(xgb_rnd.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352204a7",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Xgboost didn't out perform logistic reggression but we will try it again with grid search for the next trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e7569",
   "metadata": {},
   "source": [
    "# Trial 4\n",
    "we will try grid search this time to see if there is any change in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f0f87c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "best score 0.8501072515409712\n",
      "best score {'my_classifier__gamma': 0.5, 'my_classifier__learning_rate': 0.1, 'my_classifier__max_depth': 15, 'my_classifier__n_estimators': 450, 'my_classifier__subsample': 0.8, 'preprocessor__tfidf__max_df': 0.3, 'preprocessor__tfidf__min_df': 5, 'preprocessor__tfidf__ngram_range': (1, 5)}\n"
     ]
    }
   ],
   "source": [
    "xgb_grid = GridSearchCV(\n",
    "    full_piplineXGB, xgb_param, cv=5, verbose=1, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "xgb_grid.fit(X_train, Y_train)\n",
    "\n",
    "print('best score {}'.format(xgb_grid.best_score_))\n",
    "print('best score {}'.format(xgb_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a3643",
   "metadata": {},
   "source": [
    "Observation:\n",
    "grid search has a slightly better score but looks like xgboost isn't the optimal solution since the score dropped in submission from 85% to 81%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26e34a",
   "metadata": {},
   "source": [
    "# Trial 5\n",
    "for this trial we will try a new model we haven't used before but is typically used for Text Classification called Passive Aggressive Classifier.\n",
    "\n",
    "for the first trial we will use grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48cf7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "best score 0.8257529104901618\n",
      "best score {'my_classifier__C': 1.0, 'my_classifier__loss': 'hinge', 'preprocessor__tfidf__max_df': 0.3, 'preprocessor__tfidf__min_df': 7, 'preprocessor__tfidf__ngram_range': (1, 5)}\n"
     ]
    }
   ],
   "source": [
    "pac_param = {\n",
    "    \"preprocessor__tfidf__ngram_range\": [(1, 5),(1,3)],\n",
    "    \"preprocessor__tfidf__max_df\": np.arange(0.3,0.7),\n",
    "    \"preprocessor__tfidf__min_df\": np.arange(5,8),\n",
    "    'my_classifier__C': [1.0,1.5,2], # The regularization term C\n",
    "    'my_classifier__loss': ['hinge', 'squared_hinge'] # PA-I or PA-II\n",
    "}\n",
    "pac_grid = GridSearchCV(\n",
    "    full_piplinePAC, pac_param, cv=5, verbose=1, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "pac_grid.fit(X_train, Y_train)\n",
    "\n",
    "print('best score {}'.format(pac_grid.best_score_))\n",
    "print('best score {}'.format(pac_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d30016",
   "metadata": {},
   "source": [
    "Observation: the model gave a score of 82% , the lowest score out of all trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a56ac7",
   "metadata": {},
   "source": [
    "# Trial 6\n",
    "our final Classifier to try is Random forest, we will use grid search and use it find out the ideal parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4232912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rndf = {\n",
    "     \"preprocessor__tfidf__ngram_range\": [(1, 5),(1,3)],\n",
    "    \"preprocessor__tfidf__max_df\": [0.3],\n",
    "    \"preprocessor__tfidf__min_df\": [5],\n",
    "    \"preprocessor__tfidf__analyzer\": ['word', 'char', 'char_wb'],\n",
    "    # preprocessor__num__imputer__strategy points to preprocessor->num (a Pipeline)-> imputer -> strategy\n",
    "    'my_classifier__n_estimators': [700,750,800],  \n",
    "     # my_classifier__n_estimators points to my_classifier->n_estimators \n",
    "    'my_classifier__max_depth':[20,25,30],\n",
    "    'my_classifier__max_features': ['log2'],\n",
    "    'my_classifier__criterion': ['entropy'], \n",
    "    'my_classifier__min_samples_split':[12,13,14]   \n",
    "}\n",
    "#rf_grid = GridSearchCV(\n",
    "    #full_piplineRAN, param_rndf, cv=5, verbose=1, n_jobs=-1, \n",
    "    #scoring='roc_auc')\n",
    "\n",
    "#rf_grid.fit(X_train, Y_train)\n",
    "\n",
    "#print('best score {}'.format(rf_grid.best_score_))\n",
    "#print('best score {}'.format(rf_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248b997",
   "metadata": {},
   "source": [
    "Observation: best number of estimator is 700 which was the highest value in the parameter and we can update the parameter space based on the best values in this trial and try in the next trial space which means we will modify the paramter space for the next trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8c4b4",
   "metadata": {},
   "source": [
    "# Trial 7 \n",
    "this time we will use random search using a validation set with the updated parameter space and we added parameter analyzer to vectorizer to identify which suitable to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "258be14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "best score 0.8540925294969737\n",
      "best score {'preprocessor__tfidf__ngram_range': (1, 3), 'preprocessor__tfidf__min_df': 5, 'preprocessor__tfidf__max_df': 0.3, 'preprocessor__tfidf__analyzer': 'word', 'my_classifier__n_estimators': 750, 'my_classifier__min_samples_split': 13, 'my_classifier__max_features': 'log2', 'my_classifier__max_depth': 30, 'my_classifier__criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# Further split the original training set to a train and a validation set\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(\n",
    "    X_train, Y_train, train_size = 0.8, stratify = Y_train, random_state = 2022)\n",
    "\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X_train\n",
    "split_index = [-1 if x in X_train2.index else 0 for x in X_train.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "rd_rand= RandomizedSearchCV(\n",
    "    full_piplineRAN, param_rndf, cv=pds, verbose=1, n_jobs=-1, \n",
    "    # number of random trials\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc')\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "rd_rand.fit(X_train, Y_train)\n",
    "\n",
    "print('best score {}'.format(rd_rand.best_score_))\n",
    "print('best score {}'.format(rd_rand.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8516cf",
   "metadata": {},
   "source": [
    "Observation: Random forest has the highest score in submission and the best suited is the word analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "100e5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = df_test['id'].astype(int)\n",
    "\n",
    "submission['label'] = rd_rand.predict_proba(X_test)[:,1]\n",
    "\n",
    "submission.to_csv('rd_rand.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c960ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace5a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
